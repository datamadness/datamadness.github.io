<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-02-23T19:19:27-08:00</updated><id>/</id><title type="html">DATAmadness</title><subtitle>Exploring the world through data analysis and machine learning</subtitle><entry><title type="html">Planning Support Team Resources in Start-ups Using Statistics</title><link href="/Team-Plannning" rel="alternate" type="text/html" title="Planning Support Team Resources in Start-ups Using Statistics" /><published>2018-06-24T03:32:00-07:00</published><updated>2018-06-24T03:32:00-07:00</updated><id>/Team-Plannning</id><content type="html" xml:base="/Team-Plannning">Planning staff allocation for customer support can be challenging since great support is essential for company success but it can consume large portion of usually limited budget. This can be particularly true in smaller companies and start-ups where the case volume significantly fluctuates and the staff also needs to work on other projects besides support.

Disclaimer: Data used in this example are made up using formulas


Fortunately, basics statistics can help to guide your staffing and time allocation decisions by using readily available data from your CRM system.</content><author><name>DATAmadness</name></author><category term="Statistics" /><summary type="html">Planning staff allocation for customer support can be challenging since great support is essential for company success but it can consume large portion of usually limited budget. This can be particularly true in smaller companies and start-ups where the case volume significantly fluctuates and the staff also needs to work on other projects besides support.</summary></entry><entry><title type="html">Python function to automatically transform skewed data in Pandas DataFrame</title><link href="/Skewness_Auto_Transform" rel="alternate" type="text/html" title="Python function to automatically transform skewed data in Pandas DataFrame" /><published>2018-05-27T01:00:00-07:00</published><updated>2018-05-27T01:00:00-07:00</updated><id>/Skewness_Auto_Transform</id><content type="html" xml:base="/Skewness_Auto_Transform">&lt;p&gt;When I stumble on a cool new dataset, I often find myself excitedly prototyping a quick machine learning models to see what I can get out of the latest find. This is mostly easy thanks to plenty awesome packages such as scikit-learn or Keras. As a result, I spend most of my prototyping effort on cleaning and transforming the data.&lt;/p&gt;

&lt;p&gt;To speed up the initial transformation, I wrote a small python function that takes a Pandas DataFrame and automatically transforms any columns that exceed specified skewness. You can get it from &lt;a href=&quot;https://github.com/datamadness/Automatic-skewness-transformation-for-Pandas-DataFrame&quot;&gt;my GitHub repo&lt;/a&gt;. Specifically these two python files:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;skew_autotransform.py&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;TEST_skew_autotransform.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The first file lets you import the &lt;em&gt;skew_autotransform()&lt;/em&gt; function and use it in your project:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;from skew_autotransform import skew_autotransform
skew_autotransform(DF, include = None, exclude = None, plot = False, threshold = 1, exp = False)
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;feature-overview&quot;&gt;Feature Overview&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Analyzes all columns in Pandas DataFrame and transforms the data to improve skewness if the original skewness exceeds specified threshold&lt;/li&gt;
  &lt;li&gt;Allows to specify list of columns that should be processed or excluded&lt;/li&gt;
  &lt;li&gt;Selection between Box-Cox transformation or log / exponential transformation&lt;/li&gt;
  &lt;li&gt;Recognizes positive / negative skewness and applies appropriate transform (log / exp)&lt;/li&gt;
  &lt;li&gt;Plots before and after comparison&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;input-parameters-summary&quot;&gt;Input parameters summary&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;DF&lt;/strong&gt;: Pandas DataFrame, mandatory&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;threshold&lt;/strong&gt;: skewness threshold, default value = 1, optional&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;include&lt;/strong&gt;: list of columns to process, optional&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;exclude&lt;/strong&gt;: list of columns to exclude, optional&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;exp&lt;/strong&gt;: If true, applies log / exponential transformation, default value is False that applies Box-Cox transformation, optional&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;example-1&quot;&gt;Example #1&lt;/h4&gt;
&lt;p&gt;Import Boston housing dataset and apply Box-Cox transformation on any column that has absolute value of skewness larger than 0.5:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;import pandas as pd
import numpy as np
from skew_autotransform import skew_autotransform

#Import test dataset - Boston huosing data
from sklearn.datasets import load_boston

exampleDF = pd.DataFrame(load_boston()['data'], columns = load_boston()['feature_names'].tolist())

transformedDF = skew_autotransform(exampleDF.copy(deep=True), plot = True, exp = False, threshold = 0.5)

print('Original average skewness value was %2.2f' %(np.mean(abs(exampleDF.skew()))))
print('Average skewness after transformation is %2.2f' %(np.mean(abs(transformedDF.skew()))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:
&lt;img src=&quot;/assets/images/Skew_autotransform/example1_CRIM.png&quot; alt=&quot;image post&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt; 'CRIM' had 'positive' skewness of 5.22

 Transformation yielded skewness of 0.41
![image post](/assets/images/Skew_autotransform/example1_CRIM.png)
 ------------------------------------------------------

 'ZN' had 'positive' skewness of 2.23

 Transformation yielded skewness of 1.10
![image post](/assets/images/Skew_autotransform/example1_ZN.png)
 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'INDUS' . Skewness = 0.30
![image post](/assets/images/Skew_autotransform/example1_INDUS.png)
 ------------------------------------------------------

 'CHAS' had 'positive' skewness of 3.41

 Transformation yielded skewness of 3.41
![image post](/assets/images/Skew_autotransform/example1_CHAS.png)
 ------------------------------------------------------

 'NOX' had 'positive' skewness of 0.73

 Transformation yielded skewness of 0.36
![image post](/assets/images/Skew_autotransform/example1_NOX.png)
 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'RM' . Skewness = 0.40
![image post](/assets/images/Skew_autotransform/example1_RM.png)
 ------------------------------------------------------

 'AGE' had 'negative' skewness of -0.60

 Transformation yielded skewness of 0.94
![image post](/assets/images/Skew_autotransform/example1_AGE.png)
 ------------------------------------------------------

 'DIS' had 'positive' skewness of 1.01

 Transformation yielded skewness of 0.15
![image post](/assets/images/Skew_autotransform/example1_DIS.png)
 ------------------------------------------------------

 'RAD' had 'positive' skewness of 1.00

 Transformation yielded skewness of 0.29
![image post](/assets/images/Skew_autotransform/example1_RAD.png)
 ------------------------------------------------------

 'TAX' had 'positive' skewness of 0.67

 Transformation yielded skewness of 0.33
![image post](/assets/images/Skew_autotransform/example1_TAX.png)
 ------------------------------------------------------

 'PTRATIO' had 'negative' skewness of -0.80

 Transformation yielded skewness of 0.52
![image post](/assets/images/Skew_autotransform/example1_PTRATIO.png)
 ------------------------------------------------------

 'B' had 'negative' skewness of -2.89

 Transformation yielded skewness of -1.13
![image post](/assets/images/Skew_autotransform/example1_B.png)
 ------------------------------------------------------

 'LSTAT' had 'positive' skewness of 0.91

 Transformation yielded skewness of -0.32
![image post](/assets/images/Skew_autotransform/example1_LSTAT.png)
 ------------------------------------------------------
Original average skewness value was 1.55
Average skewness after transformation is 0.74
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;example-2&quot;&gt;Example #2&lt;/h4&gt;
&lt;p&gt;Import Boston housing dataset and apply log and exponential transformation on any column that has absolute value of skewness larger than 0.7. Exclude ‘B’ and ‘LSTAT’ column from the operation:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;import pandas as pd
import numpy as np
from skew_autotransform import skew_autotransform

#Import test dataset - Boston huosing data
from sklearn.datasets import load_boston

exampleDF = pd.DataFrame(load_boston()['data'], columns = load_boston()['feature_names'].tolist())

transformedDF = skew_autotransform(exampleDF.copy(deep=True), plot = True, exp = False, threshold = 0.5)

print('Original average skewness value was %2.2f' %(np.mean(abs(exampleDF.skew()))))
print('Average skewness after transformation is %2.2f' %(np.mean(abs(transformedDF.skew()))))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Output:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;------------------------------------------------------

 'CRIM' had 'positive' skewness of 5.22

 Transformation yielded skewness of 0.41

 ------------------------------------------------------

 'ZN' had 'positive' skewness of 2.23

 Transformation yielded skewness of 1.10

 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'INDUS' . Skewness = 0.30

 ------------------------------------------------------

 'CHAS' had 'positive' skewness of 3.41

 Transformation yielded skewness of 3.41

 ------------------------------------------------------

 'NOX' had 'positive' skewness of 0.73

 Transformation yielded skewness of 0.36

 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'RM' . Skewness = 0.40

 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'AGE' . Skewness = -0.60

 ------------------------------------------------------

 'DIS' had 'positive' skewness of 1.01

 Transformation yielded skewness of 0.15

 ------------------------------------------------------

 'RAD' had 'positive' skewness of 1.00

 Transformation yielded skewness of 0.29

 ------------------------------------------------------

 NO TRANSFORMATION APPLIED FOR 'TAX' . Skewness = 0.6

 ------------------------------------------------------

 'PTRATIO' had 'negative' skewness of -0.80

 Transformation yielded skewness of 0.52


Original average skewness value was 1.55
Average skewness after transformation is 0.92
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can see on these examples that both cases allowed me to very quickly improve the skewness of the data from 1.5 to more reasonable 0.7 and 0.9 respective. While it is not perfect, it is generally good enough for a first prototype.&lt;/p&gt;

&lt;p&gt;Enjoy!&lt;/p&gt;</content><author><name>DATAmadness</name></author><category term="Python" /><summary type="html">When I stumble on a cool new dataset, I often find myself excitedly prototyping a quick machine learning models to see what I can get out of the latest find. This is mostly easy thanks to plenty awesome packages such as scikit-learn or Keras. As a result, I spend most of my prototyping effort on cleaning and transforming the data.</summary></entry><entry><title type="html">Support Data Generator in Python</title><link href="/Support-Data-Generator" rel="alternate" type="text/html" title="Support Data Generator in Python" /><published>2018-05-27T01:00:00-07:00</published><updated>2018-05-27T01:00:00-07:00</updated><id>/Support-Data-Generator</id><content type="html" xml:base="/Support-Data-Generator">&lt;p&gt;We all have been there - it is Sunday evening, you have couple of fresh ideas for a new customer centric strategy and you want to test how it would hold up in the real world. Unfortunately, it might be hard to get real or at least somewhat realistic customer support ticket datasets for specific business models and company size.&lt;/p&gt;

&lt;p&gt;If this issue plagues you on regular basis, head to &lt;a href=&quot;https://github.com/datamadness/Support-ticket-data-generator.git&quot;&gt;this repository on my GitHub&lt;/a&gt; and download the two python files:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;main_generate_ticket_data.py&lt;/code&gt;&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;generate_daily_data.py&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;This code will allow you to input few numeric parameters and quickly generate custom support ticket datasets that reflect what you could expect in real world business operations.&lt;/p&gt;
&lt;h4 id=&quot;feature-overview&quot;&gt;Feature Overview&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;A python function to generate one year worth of support data with arbitrary amount of records(call repeatedly for x years worth of data)&lt;/li&gt;
  &lt;li&gt;Generates varied, but statistically relevant number of support tickets for each day of the year&lt;/li&gt;
  &lt;li&gt;Effects of business days vs weekends&lt;/li&gt;
  &lt;li&gt;Capable to simulate an impact of arbitrary number of busy seasons through the year(e.g. Christmas in retail or tax periods in accounting)&lt;/li&gt;
  &lt;li&gt;Simulates realistic, but easy to control statistical distribution of logged time for each support ticket / case.&lt;/li&gt;
  &lt;li&gt;Control number of customer accounts to capture desired support volume vs customer base size&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;input-parameters-summary&quot;&gt;Input parameters summary&lt;/h4&gt;

&lt;p&gt;Daily ticket volume through the year controls:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Seasonal peaks&lt;/li&gt;
  &lt;li&gt;Weekend volumes&lt;/li&gt;
  &lt;li&gt;Daily volume range (population mean, approximate min and max outliers)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Effort / logged time per ticket controls:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Median&lt;/li&gt;
  &lt;li&gt;Minimum logged time
Advanced / Optional:&lt;/li&gt;
  &lt;li&gt;Skewness factor&lt;/li&gt;
  &lt;li&gt;Degrees of freedom&lt;/li&gt;
  &lt;li&gt;Non-centrality (impacts variance and kurtosis)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Customer base controls:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Number of unique accounts&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;modelling-daily-tickets-volumes-through-the-year&quot;&gt;Modelling daily tickets volumes through the year&lt;/h4&gt;
&lt;p&gt;The number of daily tickets is typically random variable that has reasonably defined mean and variance. Moreover, the mean is likely to move through the year to reflect seasonal effects such as major holidays or annual business cycles. 
To model this behavior, the code is using truncated normal continuous variable function from scipy package to generate the number of tickets on given day:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;        #Calculate distribution function
        dist = stats.truncnorm((annual_minimum - mu) / sigma, (annual_maximum - mu) / sigma, loc=mu, scale=sigma)
        #Generate number of tickets for given day
        ticket_num = int(dist.rvs())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Simply specify the approximate minimum and maximum number of cases per day through the year. The mean &lt;code class=&quot;highlighter-rouge&quot;&gt;mu&lt;/code&gt; and variance &lt;code class=&quot;highlighter-rouge&quot;&gt;sigma&lt;/code&gt; for given day then will be automatically calculated based the seasonal effect function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;        #Calculate mean for given day
        mu = (annual_maximum - annual_minimum)/2 + seasonal_factor * (busy_level - 0.5) * ((annual_maximum - annual_minimum)/2) 
        #Calculate standard deviation for given day
        sigma = (annual_maximum - annual_minimum) * 0.1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The seasonal effect requires you to input the peak months as integer values. Here is an example for simulating two seasonal peaks in February and November:
‘busy_months = np.array([2,11])’&lt;/p&gt;

&lt;p&gt;Moreover, input &lt;code class=&quot;highlighter-rouge&quot;&gt;seasonal_factor&lt;/code&gt; (float &amp;lt;0,1&amp;gt;) allows you to control the magnitude of seasonal effects:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;0 no seasonal effects&lt;/li&gt;
  &lt;li&gt;1 maximum seasonal effects&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following plot demonstrates the annual volumes and seasonality dependency with all other parameters being constant:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;    annual_minimum = 5		# ~ annual daily minimum of tickets
    annual_maximum = 50       	# ~ annual daily maximum of tickets           
    busy_months = np.array([2,7])       # February and July are busy months
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Seasonal factor impact comparison:&lt;/em&gt;
&lt;img src=&quot;/assets/images/support_data_generator/weekly_ticket_totals_comparison.png&quot; alt=&quot;image test&quot; /&gt;&lt;/p&gt;

&lt;h6 id=&quot;impacts-of-weekends&quot;&gt;Impacts of weekends:&lt;/h6&gt;
&lt;p&gt;Weekends can have a major impact on your ticket volumes. This can be particularly pronounced if your company is B2B oriented and your clients are off work while having little to no impact if you are operating in B2C model. To model this, select the appropriate value of the weekend_factor parameter (float - &amp;lt;0,1&amp;gt;):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;0 - no ticket volume on weekends&lt;/li&gt;
  &lt;li&gt;1 - weekends are the same as business days
&lt;img src=&quot;/assets/images/support_data_generator/weekend_factor_comparison.png&quot; alt=&quot;image test&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;modelling-logged-time-for-each-opened-ticket&quot;&gt;Modelling logged time for each opened ticket&lt;/h6&gt;
&lt;p&gt;Having an understanding of how much time is spent on each ticket is a crucial part of planning your operations. Unfortunately, this data cannot be easily simulated by a simple normal distribution function. While you will be able to find mean  and variance in your real world data, you will also see few large outliers due to complex support problems resulting into positive skew. Your systems and procedures are also likely to introduce cut off limit on the low end - minimum time spent on any opened ticket. &lt;br /&gt;
Fortunately, these effects can be quite well approximated by the non-central chi-squared distribution(&lt;a href=&quot;https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution&quot;&gt;Wikipedia&lt;/a&gt;) that can generalize the k - degrees of freedom defining chi-squared function via non-centrality parameter.&lt;/p&gt;

&lt;p&gt;Probability Density Function(PDF) for non-central chi-squared function:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{X}(x;k,\lambda )=\sum _{i=0}^{\infty }{\frac {e^{-\lambda /2}(\lambda /2)^{i}}{i!}}f_{Y_{k+2i}}(x)&lt;/script&gt;

&lt;p&gt;Visualization of the non-central chi-squared PDF for various combinations of k and non-central parameter(source &lt;a href=&quot;https://en.wikipedia.org/wiki/Noncentral_chi-squared_distribution#/media/File:Chi-Squared-(nonCentral)-pdf.png&quot;&gt;en.wikipedia.com&lt;/a&gt;):
&lt;img src=&quot;/assets/images/support_data_generator/Chi-Squared-(nonCentral)-pdf.png&quot; width=&quot;600&quot; height=&quot;450&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the implemented python code, the logged time data is generated by function daily_data imported from generate_daily_data.py. This function is called by the main function with the following parameters:&lt;/p&gt;

&lt;h6 id=&quot;mandatory&quot;&gt;Mandatory:&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;size - automatically determined by the main function&lt;/li&gt;
  &lt;li&gt;logged_time_median_desired - median logged time for the population in minutes&lt;/li&gt;
  &lt;li&gt;Logged_time_minimum - minimum logged time in minutes&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;optional&quot;&gt;Optional:&lt;/h6&gt;
&lt;ul&gt;
  &lt;li&gt;skewness_factor - [0,1] value;  0 for default chi^2 distribution; 1 for large positive skew&lt;/li&gt;
  &lt;li&gt;k - degrees of freedom&lt;/li&gt;
  &lt;li&gt;nonc - non-centrality (stats typically uses lambda letter)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The following plot shows histogram and kernel density estimation(KDE) for sample of logged time data containing approximately ~8000 tickets over 12 months with minimum time = 5minutes and desired median = 20 minutes.
&lt;img src=&quot;/assets/images/support_data_generator/logged_time_distribution_plot.png&quot; alt=&quot;image test&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The same dataset was used to generate the next plot that compares the impact of skewness_factor while keeping all other parameters constant. Notice that the median does not change while the PDF does:
&lt;img src=&quot;/assets/images/support_data_generator/skewness_comparison_violin.png&quot; alt=&quot;image test&quot; /&gt;&lt;/p&gt;</content><author><name>DATAmadness</name></author><category term="Statistics" /><category term="Python" /><summary type="html">We all have been there - it is Sunday evening, you have couple of fresh ideas for a new customer centric strategy and you want to test how it would hold up in the real world. Unfortunately, it might be hard to get real or at least somewhat realistic customer support ticket datasets for specific business models and company size.</summary></entry><entry><title type="html">First Test Post</title><link href="/Test-Post" rel="alternate" type="text/html" title="First Test Post" /><published>2018-05-27T01:00:00-07:00</published><updated>2018-05-27T01:00:00-07:00</updated><id>/Test-Post</id><content type="html" xml:base="/Test-Post">&lt;p&gt;TEST POST iFrame 2 goes here&lt;/p&gt;

&lt;div style=&quot;width: 100%&quot;&gt;&lt;iframe width=&quot;100%&quot; height=&quot;600&quot; src=&quot;https://maps.google.com/maps?width=100%&amp;amp;height=600&amp;amp;hl=en&amp;amp;q=1%20Grafton%20Street%2C%20Dublin%2C%20Ireland+(My%20Business%20Name)&amp;amp;ie=UTF8&amp;amp;t=&amp;amp;z=14&amp;amp;iwloc=B&amp;amp;output=embed&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; marginheight=&quot;0&quot; marginwidth=&quot;0&quot;&gt;&lt;a href=&quot;https://www.maps.ie/create-google-map/&quot;&gt;Embed Google Map&lt;/a&gt;&lt;/iframe&gt;&lt;/div&gt;&lt;br /&gt;


&lt;p&gt;TEST - Trying to add mathjax to this post:&lt;/p&gt;
&lt;p&gt; $$ r = h = \sqrt{\frac {1} {2}} = \sqrt{\frac {N} {N+1}} \sqrt{\frac {N+1} {2N}} $$ &lt;/p&gt;
&lt;h2 id=&quot;basictagging&quot;&gt;Basic tagging&lt;/h2&gt;
&lt;p&gt;When you write a post, you can assign tags to help differentiate between categories of content. For example, you might tag some posts with &lt;code&gt;News&lt;/code&gt; and other posts with &lt;code&gt;Cycling&lt;/code&gt;, which would create two distinct categories of content listed on &lt;code&gt;/tag/news/&lt;/code&gt; and &lt;code&gt;/tag/cycling/&lt;/code&gt;, respectively.&lt;/p&gt;
&lt;p&gt;If you tag a post with both &lt;code&gt;News&lt;/code&gt; &lt;em&gt;and&lt;/em&gt; &lt;code&gt;Cycling&lt;/code&gt; - then it appears in both sections.&lt;/p&gt;
&lt;p&gt;Tag archives are like dedicated home-pages for each category of content that you have. They have their own pages, their own RSS feeds, and can support their own cover images and meta data.&lt;/p&gt;
&lt;h2 id=&quot;theprimarytag&quot;&gt;The primary tag&lt;/h2&gt;
&lt;p&gt;Inside the Ghost editor, you can drag and drop tags into a specific order. The first tag in the list is always given the most importance, and some themes will only display the primary tag (the first tag in the list) by default. So you can add the most important tag which you want to show up in your theme, but also add a bunch of related tags which are less important.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;News&lt;/strong&gt;, Cycling, Bart Stevens, Extreme Sports&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;In this example, &lt;strong&gt;News&lt;/strong&gt; is the primary tag which will be displayed by the theme, but the post will also still receive all the other tags, and show up in their respective archives.&lt;/p&gt;
&lt;h2 id=&quot;privatetags&quot;&gt;Private tags&lt;/h2&gt;
&lt;p&gt;Sometimes you may want to assign a post a specific tag, but you don't necessarily want that tag appearing in the theme or creating an archive page. In Ghost, hashtags are private and can be used for special styling.&lt;/p&gt;
&lt;p&gt;For example, if you sometimes publish posts with video content - you might want your theme to adapt and get rid of the sidebar for these posts, to give more space for an embedded video to fill the screen. In this case, you could use private tags to tell your theme what to do.&lt;/p&gt;
&lt;p&gt;&lt;mark&gt;&lt;strong&gt;News&lt;/strong&gt;, Cycling, #video&lt;/mark&gt;&lt;/p&gt;
&lt;p&gt;Here, the theme would assign the post publicly displayed tags of &lt;code&gt;News&lt;/code&gt;, and &lt;code&gt;Cycling&lt;/code&gt; - but it would also keep a private record of the post being tagged with &lt;code&gt;#video&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In your theme, you could then look for private tags conditionally and give them special formatting:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;nohighlight&quot;&gt;{{#post}}
    {{#has tag=&amp;quot;#video&amp;quot;}}
        ...markup for a nice big video post layout...
    {{else}}
        ...regular markup for a post...
    {{/has}}
{{/post}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can find documentation for theme development techniques like this and many more over on Ghost's extensive &lt;a href=&quot;https://themes.ghost.org/&quot;&gt;theme documentation&lt;/a&gt;.&lt;/p&gt;</content><author><name>DATAmadness</name></author><category term="Statistics" /><summary type="html">TEST POST iFrame 2 goes here</summary></entry></feed>